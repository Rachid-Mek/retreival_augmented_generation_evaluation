{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6309ff221c0045e4b222260bdcff323f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"islam23/ec-news-qa-dolly_test\" , split=\"train\")\n",
    "dataset = dataset.select(range(2000, 4000))\n",
    "dataset = dataset.remove_columns([\"answer\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\islam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dill\\_dill.py:414: PicklingWarning: Cannot locate reference to <class 'builtins.TiktokenTextSplitter'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "c:\\Users\\islam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dill\\_dill.py:414: PicklingWarning: Cannot pickle <class 'builtins.TiktokenTextSplitter'>: builtins.TiktokenTextSplitter has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Parameter 'function'=<function split_to_chunks at 0x00000224960EBD80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f86e0042a64e509b1aef557dcf2333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'chunks', 'num_chunks'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_text_splitter import TiktokenTextSplitter\n",
    "\n",
    "def split_to_chunks(row):\n",
    "    document = row[\"context\"]\n",
    "    chunked_articles = []\n",
    "\n",
    "    max_tokens = 512 \n",
    "    splitter = TiktokenTextSplitter(\"gpt-3.5-turbo\", trim_chunks=False)\n",
    "    chunks = splitter.chunks(document, max_tokens)\n",
    "    for chunk in chunks:\n",
    "        chunked_articles.append(chunk)\n",
    "    row[\"chunks\"] = chunked_articles\n",
    "    row[\"num_chunks\"] = len(chunked_articles)\n",
    "    return row\n",
    "dataset = dataset.map(split_to_chunks) \n",
    "dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hole initiative and so it is an extension of o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your next question comes from the line of Baty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>econd wave of 5G devices launching in late 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Additionally we saw improved QCT gross margins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for starters, what 5G has multiple near-term ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>Okay. Yeah. So, our mid point for the gross ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>hical coverage. The transition from 4G to 5G i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3782</th>\n",
       "      <td>The exact timetable might be impacted by COVID...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3783</th>\n",
       "      <td>n't break out our mix in detail, but you're co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3784</th>\n",
       "      <td>Thanks a lot. If I could, Eric, between Bob's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3785 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                context\n",
       "0     hole initiative and so it is an extension of o...\n",
       "1     Your next question comes from the line of Baty...\n",
       "2     econd wave of 5G devices launching in late 201...\n",
       "3     Additionally we saw improved QCT gross margins...\n",
       "4      for starters, what 5G has multiple near-term ...\n",
       "...                                                 ...\n",
       "3780  Okay. Yeah. So, our mid point for the gross ma...\n",
       "3781  hical coverage. The transition from 4G to 5G i...\n",
       "3782  The exact timetable might be impacted by COVID...\n",
       "3783  n't break out our mix in detail, but you're co...\n",
       "3784  Thanks a lot. If I could, Eric, between Bob's ...\n",
       "\n",
       "[3785 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chunked = pd.DataFrame(columns=[\"context\"])  # Create an empty DataFrame with one column\n",
    "\n",
    "# Iterate over each entry in the dataset and extract chunks\n",
    "for entry in dataset:\n",
    "    chunks = entry[\"chunks\"]\n",
    "    for chunk in chunks:\n",
    "        new_row = {\"context\": chunk}\n",
    "        dataset_chunked = pd.concat([dataset_chunked, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "dataset_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context'],\n",
       "    num_rows: 3785\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "dataset_chunked = Dataset.from_pandas(dataset_chunked)\n",
    "dataset_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023c84070482429d8f0f6de803c46c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'embedding'],\n",
       "    num_rows: 3785\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def _mean_pooling(model_output, attention_mask):\n",
    "  '''\n",
    "  This function takes the model output and attention mask and returns the mean pooled embeddings\n",
    "  '''\n",
    "  token_embeddings = model_output.last_hidden_state # First element of model_output contains all token embeddings\n",
    "  input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()# Create an attention mask for padding tokens\n",
    "  sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) # Sum the embeddings of the tokens over the sequence\n",
    "  sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9) # Normalize the embeddings\n",
    "  return sum_embeddings / sum_mask # Return the mean pooled embeddings\n",
    "\n",
    "def embed_text(example):\n",
    "    '''\n",
    "    this function takes a query text and returns the embeddings\n",
    "    '''\n",
    "    inputs = tokenizer(\n",
    "        example[\"context\"], padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**inputs)# Pass the tokenized query text to the model\n",
    "    pooled_embeds = _mean_pooling(model_output, inputs[\"attention_mask\"])# Get the mean pooled embeddings\n",
    "    example[\"embedding\"] = pooled_embeds.cpu().numpy()\n",
    "    return  example # Return the embeddings\n",
    "dataset_pr = dataset_chunked.map(embed_text , batched=True, batch_size=128 )\n",
    "\n",
    "dataset_pr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Upload to Qdrant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in 4 parts\n",
    "part1 = dataset_pr.select(range(0, 500))\n",
    "part2 = dataset_pr.select(range(500, 1000))\n",
    "part3 = dataset_pr.select(range(1000, 1500))\n",
    "part4 = dataset_pr.select(range(1500, 2000))\n",
    "part5 = dataset_pr.select(range(2000, 2500))\n",
    "part6 = dataset_pr.select(range(2500, 3000))\n",
    "part7 = dataset_pr.select(range(3000, 3500))\n",
    "part8 = dataset_pr.select(range(3500, 3785))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: 35 documents to the Qdrant database\n",
      "Uploaded 2 documents\n",
      "Uploaded: 35 documents to the Qdrant database\n",
      "Uploaded 37 documents\n",
      "Uploaded: 35 documents to the Qdrant database\n",
      "Uploaded 72 documents\n",
      "Uploaded: 35 documents to the Qdrant database\n",
      "Uploaded 107 documents\n",
      "Uploaded: 35 documents to the Qdrant database\n",
      "Uploaded 142 documents\n",
      "Uploaded: 35 documents to the Qdrant database\n",
      "Uploaded 177 documents\n",
      "Uploaded: 35 documents to the Qdrant database\n",
      "Uploaded 212 documents\n",
      "Uploaded: 35 documents to the Qdrant database\n",
      "Uploaded 247 documents\n",
      "Uploaded: 5 documents to the Qdrant database\n",
      "Uploaded 282 documents\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from qdrant_client.http import models\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "class QdrantU:\n",
    "        def __init__(self, collection_name):\n",
    "            self.client = QdrantClient(\n",
    "                url=\"https://5c32ac64-b1f7-4665-91eb-e321a98c02f6.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
    "                api_key=\"Wd_RTregmznFMCyDLagJHM_7a5TjJJuFLVTuMgfjQD44-BHLnhYbUg\",\n",
    "            )\n",
    "            self.collection_name = collection_name\n",
    "\n",
    "        def _upload_documents_to_Qdrant(self, data):\n",
    "            points = []\n",
    "            for   content , embedding in zip( data[\"context\"] , data[\"embedding\"]):\n",
    "                new_id = str(uuid.uuid4())  # Generate a new UUID for each document\n",
    "                point = models.PointStruct(\n",
    "                    id=new_id,\n",
    "                    vector=embedding,\n",
    "                    payload={\n",
    "                        \"chunk\": content,\n",
    "                    }\n",
    "                )\n",
    "                points.append(point)\n",
    "\n",
    "            self.client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=points\n",
    "            )\n",
    "\n",
    "            print(\"Uploaded:\", len(data[\"embedding\"]), \"documents to the Qdrant database\")\n",
    "\n",
    "        def upload_to_Qdrant(self, data, batch_size=35):\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                batch = data[i:i + batch_size]\n",
    "                self._upload_documents_to_Qdrant(batch )\n",
    "                print(f\"Uploaded {i + len(batch)} documents\")\n",
    "        def close(self):\n",
    "            self.client.close()\n",
    " \n",
    "\n",
    "        \n",
    "collection_name = \"Evaluation_support\"\n",
    "# client = QdrantClient(\n",
    "#                 url=\"https://5c32ac64-b1f7-4665-91eb-e321a98c02f6.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
    "#                 api_key=\"Wd_RTregmznFMCyDLagJHM_7a5TjJJuFLVTuMgfjQD44-BHLnhYbUg\",\n",
    "#             )\n",
    "# client.create_collection(\n",
    "#     collection_name= collection_name,\n",
    "#     vectors_config=models.VectorParams(\n",
    "#         size=768,\n",
    "#         distance=models.Distance.COSINE,\n",
    "#     ),\n",
    "# )\n",
    "client = QdrantU(collection_name)\n",
    "client.upload_to_Qdrant(part8)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('rachid16/Retrival_evaluation_dataset', split = 'train')\n",
    "dataset = dataset.select(range(750))\n",
    "df = dataset.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_by_indices(docs, indices):\n",
    "    \"\"\"\n",
    "    Retrieve document contexts from a list of indexed documents based on provided indices.\n",
    "\n",
    "    Args:\n",
    "    - docs (list): List of documents.\n",
    "    - indices (list): List of indices corresponding to the desired documents.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of document contexts corresponding to the provided indices.\n",
    "    \"\"\"\n",
    "    return [docs[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_relevant_documents(documents, chunks):\n",
    "  \"\"\"\n",
    "  This function identifies relevant documents based on chunks.\n",
    "\n",
    "  Args:\n",
    "      documents: A list of strings representing documents.\n",
    "      chunks: A list of strings representing document chunks.\n",
    "\n",
    "  Returns:\n",
    "      A list of 0s and 1s, where 1 indicates a relevant document and 0 indicates an irrelevant document.\n",
    "  \"\"\"\n",
    "  # Initialize an empty list to store relevance scores (0 or 1).\n",
    "  relevance_scores = []\n",
    "\n",
    "  for document in documents:\n",
    "    # Check if any chunk of the document exists in the chunk_indices dictionary.\n",
    "    is_relevant = any(chunk in document for chunk in chunks)\n",
    "\n",
    "    # Assign 1 for relevant document and 0 for irrelevant document.\n",
    "    relevance_scores.append(1 if is_relevant else 0)\n",
    "\n",
    "  return relevance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query_text):\n",
    "  '''\n",
    "  this function takes a query text and returns the embeddings\n",
    "  '''\n",
    "  query_inputs = tokenizer(\n",
    "      query_text, # Tokenize the query text\n",
    "      padding=True, # Pad the query text to the maximum length\n",
    "      truncation=True, # Truncate the query text to the maximum length if it exceeds the maximum length\n",
    "      return_tensors=\"pt\" # Return the tokenized query text as PyTorch tensors\n",
    "  )\n",
    "\n",
    "  with torch.no_grad():\n",
    "      query_model_output = model(**query_inputs) # Pass the tokenized query text to the model\n",
    "\n",
    "  query_embedding = _mean_pooling(query_model_output, query_inputs[\"attention_mask\"]) # Get the mean pooled embeddings\n",
    "\n",
    "  return query_embedding # Return the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, limit=10):\n",
    "    '''\n",
    "      This function retrieves the vectors from the Qdrant database based on the query by similarity search\n",
    "    '''\n",
    "    query_vector = embed_query(query_text=query)\n",
    "    query_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector[0].tolist(),\n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each row in the dataframe, identify relevant documents based on chunks\n",
    "import cohere\n",
    "\n",
    "def store_identify_relevant_documents(row):\n",
    "    query = row['question']\n",
    "    search_results = search(query, limit=50)\n",
    "    docs = list(set([result.payload['chunk'] for result in search_results]))\n",
    "    apiKey = 'Uk9ecA7NyRDjlkow5TjszJai5cQeJSTakfTQG0CT'\n",
    "    co = cohere.Client(apiKey)\n",
    "    rerank_docs = co.rerank(\n",
    "        query=query, documents=docs, top_n=row['num_chunks'], model=\"rerank-english-v3.0\"\n",
    "    )\n",
    "    indices = [result.index for result in rerank_docs.results]\n",
    "    documents = get_docs_by_indices(docs, indices)\n",
    "    relevent_docs = identify_relevant_documents(documents, row['chunks'])\n",
    "    row['relvent_doc'] = relevent_docs\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(store_identify_relevant_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\"Retrival_evaluation_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
