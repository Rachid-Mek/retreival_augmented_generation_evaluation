{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rachi\\work\\RAG_EVALUATION\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset , load_dataset\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\rachi\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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 --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datasets import Dataset  # Assuming 'datasets' refers to the Hugging Face's datasets library.\n",
    "\n",
    "# def concatenate_csv_files():\n",
    "#     # Read the csv files\n",
    "#     file_paths = [\n",
    "#         'Data/data_answers_eval_F_1.csv',\n",
    "#         'Data/data_answers_eval_F_2.csv',\n",
    "#         'Data/data_answers_eval_F_3.csv',\n",
    "#         'Data/data_answers_eval_F_4.csv',\n",
    "#         'Data/data_answers_eval_F_5.csv',\n",
    "#         'Data/data_answers_eval_F_6.csv',\n",
    "#         'Data/data_answers_eval_F_7.csv',\n",
    "#         'Data/data_answers_eval_F_8.csv',\n",
    "#         'Data/data_answers_eval_F_9.csv'\n",
    "#     ]\n",
    "    \n",
    "#     dataframes = [pd.read_csv(file) for file in file_paths]\n",
    "\n",
    "#     # Concatenate the dataframes\n",
    "#     df = pd.concat(dataframes, ignore_index=True)  # ignore_index=True to reset the index\n",
    "    \n",
    "#     # Turn df into a dataset\n",
    "#     dataset = Dataset.from_pandas(df)\n",
    "\n",
    "#     # Push the dataset to the hub\n",
    "#     dataset.push_to_hub('evaluation_data')\n",
    "\n",
    "# # Call the function to execute\n",
    "# concatenate_csv_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 578/578 [00:00<?, ?B/s] \n",
      "Downloading data: 100%|██████████| 3.99M/3.99M [00:01<00:00, 3.04MB/s]\n",
      "Generating train split: 100%|██████████| 2000/2000 [00:00<00:00, 18915.84 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>Finetuned_answer</th>\n",
       "      <th>BERTScore_Precision</th>\n",
       "      <th>BERTScore_Recall</th>\n",
       "      <th>BERTScore_F1</th>\n",
       "      <th>entailement_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the expected increase in competition i...</td>\n",
       "      <td>ut content goes to the Internet. Internet goes...</td>\n",
       "      <td>I do not know the expected increase in compet...</td>\n",
       "      <td>The CEO, Mike Sievert, mentioned that the comp...</td>\n",
       "      <td>0.4909</td>\n",
       "      <td>0.6611</td>\n",
       "      <td>0.5634</td>\n",
       "      <td>0.140802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the revenue generated by Baidu's AI Cl...</td>\n",
       "      <td>urrent and preliminary view, which is subject ...</td>\n",
       "      <td>The revenue generated by Baidu's AI Cloud bus...</td>\n",
       "      <td>Baidu's AI Cloud business continued to outperf...</td>\n",
       "      <td>0.4492</td>\n",
       "      <td>0.7249</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>0.013964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was the blended gross margin in the secon...</td>\n",
       "      <td>alf of 2020, which is significantly higher com...</td>\n",
       "      <td>The blended gross margin in the second quarte...</td>\n",
       "      <td>According to the provided context, the blended...</td>\n",
       "      <td>0.6384</td>\n",
       "      <td>0.9274</td>\n",
       "      <td>0.7562</td>\n",
       "      <td>0.110550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the expected revenue for Twist's data ...</td>\n",
       "      <td>terms of numbers and bio customers and also th...</td>\n",
       "      <td>I do not know the expected revenue for Twist'...</td>\n",
       "      <td>Based on the provided context, it seems that t...</td>\n",
       "      <td>0.3980</td>\n",
       "      <td>0.7282</td>\n",
       "      <td>0.5147</td>\n",
       "      <td>0.036343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the estimated cash flow contribution i...</td>\n",
       "      <td>g on. We've got relationships with 80 universi...</td>\n",
       "      <td>The estimated cash flow contribution in 2025 ...</td>\n",
       "      <td>The CEO, Darren Woods, mentioned that the comp...</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>0.7462</td>\n",
       "      <td>0.7016</td>\n",
       "      <td>0.001845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the expected increase in competition i...   \n",
       "1  What is the revenue generated by Baidu's AI Cl...   \n",
       "2  What was the blended gross margin in the secon...   \n",
       "3  What is the expected revenue for Twist's data ...   \n",
       "4  What is the estimated cash flow contribution i...   \n",
       "\n",
       "                                             context  \\\n",
       "0  ut content goes to the Internet. Internet goes...   \n",
       "1  urrent and preliminary view, which is subject ...   \n",
       "2  alf of 2020, which is significantly higher com...   \n",
       "3  terms of numbers and bio customers and also th...   \n",
       "4  g on. We've got relationships with 80 universi...   \n",
       "\n",
       "                                              answer  \\\n",
       "0   I do not know the expected increase in compet...   \n",
       "1   The revenue generated by Baidu's AI Cloud bus...   \n",
       "2   The blended gross margin in the second quarte...   \n",
       "3   I do not know the expected revenue for Twist'...   \n",
       "4   The estimated cash flow contribution in 2025 ...   \n",
       "\n",
       "                                    Finetuned_answer  BERTScore_Precision  \\\n",
       "0  The CEO, Mike Sievert, mentioned that the comp...               0.4909   \n",
       "1  Baidu's AI Cloud business continued to outperf...               0.4492   \n",
       "2  According to the provided context, the blended...               0.6384   \n",
       "3  Based on the provided context, it seems that t...               0.3980   \n",
       "4  The CEO, Darren Woods, mentioned that the comp...               0.6621   \n",
       "\n",
       "   BERTScore_Recall  BERTScore_F1  entailement_score  \n",
       "0            0.6611        0.5634           0.140802  \n",
       "1            0.7249        0.5547           0.013964  \n",
       "2            0.9274        0.7562           0.110550  \n",
       "3            0.7282        0.5147           0.036343  \n",
       "4            0.7462        0.7016           0.001845  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('rachid16/ft_bert_benchmark', split ='train')\n",
    "dataset_df = pd.DataFrame(dataset)\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_text(results):\n",
    "  \"\"\"\n",
    "  Extracts informative text from machine responses, handling specific formatting using regular expressions.\n",
    "\n",
    "  Args:\n",
    "      results: A list of strings containing machine responses.\n",
    "\n",
    "  Returns:\n",
    "      A list of strings containing the extracted text from each response.\n",
    "  \"\"\"\n",
    "  pattern = r\"(According to|Based on)\\s+.*?,\"  # Matches \"According to\" or \"Based on\" followed by anything until a comma (,)\n",
    "  extracted_text = []\n",
    "  for result in results:\n",
    "    # Substitute the matched pattern with an empty string\n",
    "    cleaned_text = re.sub(pattern, \"\", result, flags=re.IGNORECASE)\n",
    "    extracted_text.append(cleaned_text.strip())  # Remove leading/trailing whitespaces\n",
    "\n",
    "  return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_texts=list(dataset_df[\"answer\"])\n",
    "machine_results_Finetuned= list(dataset_df['Finetuned_answer'])\n",
    "questions = list(dataset_df['question'])\n",
    "context = list(dataset_df['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_results_Finetuned_Copy = extract_text(machine_results_Finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the column Finetuned_answer by machine_results_Finetuned_Copy\n",
    "dataset_df['Finetuned_answer'] = machine_results_Finetuned_Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rachi\\work\\RAG_EVALUATION\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rachi\\work\\RAG_EVALUATION\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rachi\\.cache\\huggingface\\hub\\models--MoritzLaurer--DeBERTa-v3-base-mnli-fever-anli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\rachi\\work\\RAG_EVALUATION\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load pre-trained DeBERTa model and tokenizer (fine-tuned for MNLI)\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9578638672828674,\n",
       " 0.9527500867843628,\n",
       " 0.9002301096916199,\n",
       " 0.8854920268058777,\n",
       " 0.9787181615829468,\n",
       " 0.9492406845092773,\n",
       " 0.9085161685943604,\n",
       " 0.45014968514442444,\n",
       " 0.1774141490459442,\n",
       " 0.7958884835243225,\n",
       " 0.5668376088142395,\n",
       " 0.04102431237697601,\n",
       " 0.7582909464836121,\n",
       " 0.24299491941928864,\n",
       " 0.7034161686897278,\n",
       " 0.5403362512588501,\n",
       " 0.527260959148407,\n",
       " 0.16124361753463745,\n",
       " 0.5936254858970642,\n",
       " 0.16606934368610382,\n",
       " 0.995669960975647,\n",
       " 0.5244122743606567,\n",
       " 0.7243959307670593,\n",
       " 0.29930949211120605,\n",
       " 0.9289849996566772,\n",
       " 0.3559146821498871,\n",
       " 0.043598473072052,\n",
       " 0.04242722690105438,\n",
       " 0.06054333597421646,\n",
       " 0.8747493028640747,\n",
       " 0.5966969132423401,\n",
       " 0.45875343680381775,\n",
       " 0.36484503746032715,\n",
       " 0.9497244954109192,\n",
       " 0.39255502820014954,\n",
       " 0.9769832491874695,\n",
       " 0.9106320142745972,\n",
       " 0.4343370795249939,\n",
       " 0.15188699960708618,\n",
       " 0.20587171614170074,\n",
       " 0.8373094201087952,\n",
       " 0.7227925062179565,\n",
       " 0.9562168121337891,\n",
       " 0.881180465221405,\n",
       " 0.8196017146110535,\n",
       " 0.9979442954063416,\n",
       " 0.3420686423778534,\n",
       " 0.473405659198761,\n",
       " 0.1499221920967102,\n",
       " 0.9844815731048584,\n",
       " 0.780846118927002,\n",
       " 0.6300247311592102,\n",
       " 0.9968494772911072,\n",
       " 0.15715396404266357,\n",
       " 0.8082570433616638,\n",
       " 0.8446744680404663,\n",
       " 0.9700770974159241,\n",
       " 0.9295238852500916,\n",
       " 0.9703117609024048,\n",
       " 0.9876054525375366,\n",
       " 0.7077383995056152,\n",
       " 0.8874304890632629,\n",
       " 0.625006914138794,\n",
       " 0.7489429116249084,\n",
       " 0.8129672408103943,\n",
       " 0.04721859097480774,\n",
       " 0.5959605574607849,\n",
       " 0.9822080731391907,\n",
       " 0.748457133769989,\n",
       " 0.8270378112792969,\n",
       " 0.13247843086719513,\n",
       " 0.23804216086864471,\n",
       " 0.5760820508003235,\n",
       " 0.7304872870445251,\n",
       " 0.14593127369880676,\n",
       " 0.13779567182064056,\n",
       " 0.558453381061554,\n",
       " 0.9394084215164185,\n",
       " 0.8597367405891418,\n",
       " 0.8804664611816406,\n",
       " 0.4696495234966278,\n",
       " 0.7170483469963074,\n",
       " 0.07523997873067856,\n",
       " 0.9441436529159546,\n",
       " 0.9519206881523132,\n",
       " 0.697773814201355,\n",
       " 0.9915345311164856,\n",
       " 0.9718724489212036,\n",
       " 0.6110252737998962,\n",
       " 0.04490191489458084,\n",
       " 0.7630492448806763,\n",
       " 0.9703455567359924,\n",
       " 0.6028120517730713,\n",
       " 0.8543396592140198,\n",
       " 0.10294315218925476,\n",
       " 0.8244953155517578,\n",
       " 0.43851563334465027,\n",
       " 0.396055668592453,\n",
       " 0.3326760232448578,\n",
       " 0.18454617261886597,\n",
       " 0.1882825493812561,\n",
       " 0.860295295715332,\n",
       " 0.9526911973953247,\n",
       " 0.968006432056427,\n",
       " 0.4757835268974304,\n",
       " 0.4651399850845337,\n",
       " 0.6798900961875916,\n",
       " 0.36181047558784485,\n",
       " 0.5411001443862915,\n",
       " 0.9823819994926453,\n",
       " 0.9579647779464722,\n",
       " 0.8609387278556824,\n",
       " 0.3974066376686096,\n",
       " 0.6868413090705872,\n",
       " 0.6274785995483398,\n",
       " 0.9855324625968933,\n",
       " 0.6619833707809448,\n",
       " 0.8934762477874756,\n",
       " 0.4992254376411438,\n",
       " 0.8968561887741089,\n",
       " 0.11849924921989441,\n",
       " 0.14040133357048035,\n",
       " 0.4330025911331177,\n",
       " 0.8265037536621094,\n",
       " 0.8789121508598328,\n",
       " 0.9893875122070312,\n",
       " 0.7049285173416138,\n",
       " 0.3965221345424652,\n",
       " 0.5755167007446289,\n",
       " 0.7132264375686646,\n",
       " 0.8647336363792419,\n",
       " 0.30054953694343567,\n",
       " 0.8260438442230225,\n",
       " 0.91634202003479,\n",
       " 0.09834863245487213,\n",
       " 0.996980607509613,\n",
       " 0.8317447900772095,\n",
       " 0.9049541354179382,\n",
       " 0.6310982704162598,\n",
       " 0.905691385269165,\n",
       " 0.9410335421562195,\n",
       " 0.8483341336250305,\n",
       " 0.5833121538162231,\n",
       " 0.3343806564807892,\n",
       " 0.9836894869804382,\n",
       " 0.9811441898345947,\n",
       " 0.8491498827934265,\n",
       " 0.6494103670120239,\n",
       " 0.5439392924308777,\n",
       " 0.9359622001647949,\n",
       " 0.9383515119552612,\n",
       " 0.6960372924804688,\n",
       " 0.7781857252120972,\n",
       " 0.9531424641609192,\n",
       " 0.1330278515815735,\n",
       " 0.37864723801612854,\n",
       " 0.77802973985672,\n",
       " 0.8125724792480469,\n",
       " 0.6284354329109192,\n",
       " 0.03792432323098183,\n",
       " 0.571239173412323,\n",
       " 0.4045218229293823,\n",
       " 0.8834095597267151,\n",
       " 0.03563080355525017,\n",
       " 0.5024738907814026,\n",
       " 0.9446657299995422,\n",
       " 0.9155169129371643,\n",
       " 0.3565737009048462,\n",
       " 0.6979677081108093,\n",
       " 0.9930376410484314,\n",
       " 0.8194822669029236,\n",
       " 0.9688922166824341,\n",
       " 0.5761882066726685,\n",
       " 0.713981032371521,\n",
       " 0.9688275456428528,\n",
       " 0.8994747996330261,\n",
       " 0.9969528913497925,\n",
       " 0.5073513984680176,\n",
       " 0.7104089260101318,\n",
       " 0.5507566332817078,\n",
       " 0.5745939016342163,\n",
       " 0.5602169036865234,\n",
       " 0.9313862919807434,\n",
       " 0.3934496343135834,\n",
       " 0.20467646420001984,\n",
       " 0.44803211092948914,\n",
       " 0.29203668236732483,\n",
       " 0.46745046973228455,\n",
       " 0.9249143600463867,\n",
       " 0.8539680242538452,\n",
       " 0.9834514856338501,\n",
       " 0.9972871541976929,\n",
       " 0.6039122343063354,\n",
       " 0.2723486125469208,\n",
       " 0.9088621139526367,\n",
       " 0.6072846055030823,\n",
       " 0.34588801860809326,\n",
       " 0.4708227515220642,\n",
       " 0.5949649810791016,\n",
       " 0.9489845633506775,\n",
       " 0.9541859030723572,\n",
       " 0.4170682728290558,\n",
       " 0.3374898433685303,\n",
       " 0.2583295404911041,\n",
       " 0.872416079044342,\n",
       " 0.0036795414052903652,\n",
       " 0.7457410097122192,\n",
       " 0.7052789330482483,\n",
       " 0.6559286117553711,\n",
       " 0.6316087245941162,\n",
       " 0.6845142841339111,\n",
       " 0.2310331016778946,\n",
       " 0.0584142692387104,\n",
       " 0.4198293089866638,\n",
       " 0.8900120258331299,\n",
       " 0.9262588620185852,\n",
       " 0.8573017716407776,\n",
       " 0.7411121129989624,\n",
       " 0.9543553590774536,\n",
       " 0.9172878861427307,\n",
       " 0.9028762578964233,\n",
       " 0.6974529027938843,\n",
       " 0.715153694152832,\n",
       " 0.8548682332038879,\n",
       " 0.8449695706367493,\n",
       " 0.9200642704963684,\n",
       " 0.9313127994537354,\n",
       " 0.7868980765342712,\n",
       " 0.028613701462745667,\n",
       " 0.7051410675048828,\n",
       " 0.8047532439231873,\n",
       " 0.6335848569869995,\n",
       " 0.7447205781936646,\n",
       " 0.8201206922531128,\n",
       " 0.8148319721221924,\n",
       " 0.0780891478061676,\n",
       " 0.6228044033050537,\n",
       " 0.5205069184303284,\n",
       " 0.5910215973854065,\n",
       " 0.9740970134735107,\n",
       " 0.9812319874763489,\n",
       " 0.9355268478393555,\n",
       " 0.7016453146934509,\n",
       " 0.287274032831192,\n",
       " 0.9568987488746643,\n",
       " 0.20612317323684692,\n",
       " 0.7456667423248291,\n",
       " 0.2962774634361267,\n",
       " 0.2739219665527344,\n",
       " 0.09511733055114746,\n",
       " 0.8141885995864868,\n",
       " 0.9051146507263184,\n",
       " 0.35787680745124817,\n",
       " 0.3240578770637512,\n",
       " 0.3769563138484955,\n",
       " 0.9396286606788635,\n",
       " 0.8054052591323853,\n",
       " 0.9864821434020996,\n",
       " 0.9280648231506348,\n",
       " 0.8461804389953613,\n",
       " 0.33629921078681946,\n",
       " 0.8913272619247437,\n",
       " 0.9811825156211853,\n",
       " 0.7053781747817993,\n",
       " 0.898529052734375,\n",
       " 0.9623948335647583,\n",
       " 0.8405702710151672,\n",
       " 0.6579034924507141,\n",
       " 0.7116936445236206,\n",
       " 0.19100995361804962,\n",
       " 0.9082313179969788,\n",
       " 0.19689840078353882,\n",
       " 0.0951966866850853,\n",
       " 0.6256612539291382,\n",
       " 0.9740388989448547,\n",
       " 0.11326248198747635,\n",
       " 0.18743498623371124,\n",
       " 0.3310016095638275,\n",
       " 0.9385464191436768,\n",
       " 0.9116895794868469,\n",
       " 0.5034152269363403,\n",
       " 0.7918077707290649,\n",
       " 0.8575407862663269,\n",
       " 0.3401944637298584,\n",
       " 0.9533037543296814,\n",
       " 0.5658240914344788,\n",
       " 0.9164230823516846,\n",
       " 0.5478932857513428,\n",
       " 0.7372269630432129,\n",
       " 0.958245575428009,\n",
       " 0.9553154706954956,\n",
       " 0.17883160710334778,\n",
       " 0.8847442269325256,\n",
       " 0.8921508193016052,\n",
       " 0.5784865617752075,\n",
       " 0.8988582491874695,\n",
       " 0.9807904958724976,\n",
       " 0.30252841114997864,\n",
       " 0.8273059129714966,\n",
       " 0.7033059000968933,\n",
       " 0.6840826272964478,\n",
       " 0.423366904258728,\n",
       " 0.3670928478240967,\n",
       " 0.873935878276825,\n",
       " 0.7694642543792725,\n",
       " 0.053096428513526917,\n",
       " 0.8991907238960266,\n",
       " 0.1391550451517105,\n",
       " 0.8720986843109131,\n",
       " 0.2677452266216278,\n",
       " 0.8394582271575928,\n",
       " 0.65057373046875,\n",
       " 0.42522481083869934,\n",
       " 0.9524400234222412,\n",
       " 0.3871954381465912,\n",
       " 0.8637686967849731,\n",
       " 0.9008540511131287,\n",
       " 0.9591823816299438,\n",
       " 0.9217497706413269,\n",
       " 0.5238829255104065,\n",
       " 0.8428171277046204,\n",
       " 0.12725919485092163,\n",
       " 0.8813562989234924,\n",
       " 0.14764776825904846,\n",
       " 0.8853855133056641,\n",
       " 0.923616886138916,\n",
       " 0.8404433727264404,\n",
       " 0.0893784761428833,\n",
       " 0.9182831048965454,\n",
       " 0.6025457382202148,\n",
       " 0.14215216040611267,\n",
       " 0.990338146686554,\n",
       " 0.17700371146202087,\n",
       " 0.18144087493419647,\n",
       " 0.9645922183990479,\n",
       " 0.9079087972640991,\n",
       " 0.6029044389724731,\n",
       " 0.9178553819656372,\n",
       " 0.9147434234619141,\n",
       " 0.5985813140869141,\n",
       " 0.8449695706367493,\n",
       " 0.8129963874816895,\n",
       " 0.9823417067527771,\n",
       " 0.9386312961578369,\n",
       " 0.9625416398048401,\n",
       " 0.21606381237506866,\n",
       " 0.1721305102109909,\n",
       " 0.24141842126846313,\n",
       " 0.9918163418769836,\n",
       " 0.9515954852104187,\n",
       " 0.9928203225135803,\n",
       " 0.9352036118507385,\n",
       " 0.9126158356666565,\n",
       " 0.962035059928894,\n",
       " 0.8675938844680786,\n",
       " 0.9973461627960205,\n",
       " 0.8965470790863037,\n",
       " 0.512269139289856,\n",
       " 0.31167277693748474,\n",
       " 0.2325020134449005,\n",
       " 0.9293466210365295,\n",
       " 0.09940879791975021,\n",
       " 0.31216928362846375,\n",
       " 0.10212315618991852,\n",
       " 0.18908825516700745,\n",
       " 0.6802599430084229,\n",
       " 0.9528086185455322,\n",
       " 0.916144847869873,\n",
       " 0.9291850328445435,\n",
       " 0.8988518714904785,\n",
       " 0.911385178565979,\n",
       " 0.9508198499679565,\n",
       " 0.29514580965042114,\n",
       " 0.2899094521999359,\n",
       " 0.32736891508102417,\n",
       " 0.8802856206893921,\n",
       " 0.8946834206581116,\n",
       " 0.8029089570045471,\n",
       " 0.7865915298461914,\n",
       " 0.5491071343421936,\n",
       " 0.624095618724823,\n",
       " 0.8594179153442383,\n",
       " 0.027742505073547363,\n",
       " 0.28438061475753784,\n",
       " 0.4727124273777008,\n",
       " 0.993653416633606,\n",
       " 0.9223219156265259,\n",
       " 0.8123737573623657,\n",
       " 0.88275545835495,\n",
       " 0.5635302066802979,\n",
       " 0.9339810609817505,\n",
       " 0.4043006896972656,\n",
       " 0.6983889937400818,\n",
       " 0.6011961102485657,\n",
       " 0.3661157488822937,\n",
       " 0.3462103307247162,\n",
       " 0.7466028332710266,\n",
       " 0.9203013181686401,\n",
       " 0.9393047094345093,\n",
       " 0.48413628339767456,\n",
       " 0.8974729776382446,\n",
       " 0.7722135782241821,\n",
       " 0.40435779094696045,\n",
       " 0.017921896651387215,\n",
       " 0.9149642586708069,\n",
       " 0.6738738417625427,\n",
       " 0.9164254069328308,\n",
       " 0.9238108396530151,\n",
       " 0.33480775356292725,\n",
       " 0.9702785015106201,\n",
       " 0.5203710794448853,\n",
       " 0.9495298266410828,\n",
       " 0.052796900272369385,\n",
       " 0.9412797689437866,\n",
       " 0.8241516947746277,\n",
       " 0.6099967956542969,\n",
       " 0.7991124391555786,\n",
       " 0.14516189694404602,\n",
       " 0.4614555537700653,\n",
       " 0.2694782316684723,\n",
       " 0.12482468783855438,\n",
       " 0.8323431611061096,\n",
       " 0.9769376516342163,\n",
       " 0.39285311102867126,\n",
       " 0.539703905582428,\n",
       " 0.8853746652603149,\n",
       " 0.009794685989618301,\n",
       " 0.2421065717935562,\n",
       " 0.8945732116699219,\n",
       " 0.9537844657897949,\n",
       " 0.9408655762672424,\n",
       " 0.008836901746690273,\n",
       " 0.939724862575531,\n",
       " 0.8780936598777771,\n",
       " 0.22618381679058075,\n",
       " 0.900134801864624,\n",
       " 0.94184809923172,\n",
       " 0.0859404131770134,\n",
       " 0.9459428787231445,\n",
       " 0.8010371327400208,\n",
       " 0.5928835272789001,\n",
       " 0.04692186415195465,\n",
       " 0.8946133852005005,\n",
       " 0.400878369808197,\n",
       " 0.8991196155548096,\n",
       " 0.48102840781211853,\n",
       " 0.6075833439826965,\n",
       " 0.33917728066444397,\n",
       " 0.7786483764648438,\n",
       " 0.11091592907905579,\n",
       " 0.9370684623718262,\n",
       " 0.9175295829772949,\n",
       " 0.6893719434738159,\n",
       " 0.8365383148193359,\n",
       " 0.86370450258255,\n",
       " 0.9939129948616028,\n",
       " 0.6937540769577026,\n",
       " 0.3135330080986023,\n",
       " 0.9592668414115906,\n",
       " 0.9385477900505066,\n",
       " 0.6603363752365112,\n",
       " 0.47661587595939636,\n",
       " 0.6720741391181946,\n",
       " 0.29961833357810974,\n",
       " 0.5314289927482605,\n",
       " 0.0687774270772934,\n",
       " 0.9502602815628052,\n",
       " 0.8654107451438904,\n",
       " 0.5353519320487976,\n",
       " 0.9266835451126099,\n",
       " 0.3876175582408905,\n",
       " 0.7455418109893799,\n",
       " 0.5154874920845032,\n",
       " 0.43636760115623474,\n",
       " 0.5676842927932739,\n",
       " 0.2551492154598236,\n",
       " 0.5540587306022644,\n",
       " 0.6900004744529724,\n",
       " 0.9978533387184143,\n",
       " 0.07742445915937424,\n",
       " 0.8977568745613098,\n",
       " 0.250385046005249,\n",
       " 0.779059112071991,\n",
       " 0.7537727952003479,\n",
       " 0.5288688540458679,\n",
       " 0.382371187210083,\n",
       " 0.8726690411567688,\n",
       " 0.9602153897285461,\n",
       " 0.2488226741552353,\n",
       " 0.7968422770500183,\n",
       " 0.7908016443252563,\n",
       " 0.8432503342628479,\n",
       " 0.9564540386199951,\n",
       " 0.30538806319236755,\n",
       " 0.5056284666061401,\n",
       " 0.9391775727272034,\n",
       " 0.6199425458908081,\n",
       " 0.09974608570337296,\n",
       " 0.940916121006012,\n",
       " 0.108108751475811,\n",
       " 0.6523563265800476,\n",
       " 0.7820113897323608,\n",
       " 0.89316725730896,\n",
       " 0.5005958080291748,\n",
       " 0.9609580039978027,\n",
       " 0.5427890419960022,\n",
       " 0.14823214709758759,\n",
       " 0.9197782874107361,\n",
       " 0.9866438508033752,\n",
       " 0.6562696695327759,\n",
       " 0.9667880535125732,\n",
       " 0.8521041870117188,\n",
       " 0.6124321818351746,\n",
       " 0.23217955231666565,\n",
       " 0.6730417609214783,\n",
       " 0.9088352918624878,\n",
       " 0.7726818919181824,\n",
       " 0.9763011336326599,\n",
       " 0.8636332750320435,\n",
       " 0.8951559066772461,\n",
       " 0.40317580103874207,\n",
       " 0.3676551282405853,\n",
       " 0.5850273966789246,\n",
       " 0.982309877872467,\n",
       " 0.8779245615005493,\n",
       " 0.9202444553375244,\n",
       " 0.9663172960281372,\n",
       " 0.9731460809707642,\n",
       " 0.8920662999153137,\n",
       " 0.1401459276676178,\n",
       " 0.4700646698474884,\n",
       " 0.15621237456798553,\n",
       " 0.9121463894844055,\n",
       " 0.6700582504272461,\n",
       " 0.7984507083892822,\n",
       " 0.8065662384033203,\n",
       " 0.9230164289474487,\n",
       " 0.9014360308647156,\n",
       " 0.11696392297744751,\n",
       " 0.42945531010627747,\n",
       " 0.8005140423774719,\n",
       " 0.7132128477096558,\n",
       " 0.005172260571271181,\n",
       " 0.5351591110229492,\n",
       " 0.992374837398529,\n",
       " 0.9111881852149963,\n",
       " 0.028917837888002396,\n",
       " 0.9689518809318542,\n",
       " 0.9183875322341919,\n",
       " 0.8750830888748169,\n",
       " 0.3460591435432434,\n",
       " 0.6010103225708008,\n",
       " 0.9677504301071167,\n",
       " 0.10534803569316864,\n",
       " 0.996086597442627,\n",
       " 0.6634196639060974,\n",
       " 0.155126690864563,\n",
       " 0.9042730331420898,\n",
       " 0.81172114610672,\n",
       " 0.9215680360794067,\n",
       " 0.822493851184845,\n",
       " 0.807157039642334,\n",
       " 0.8823609948158264,\n",
       " 0.3769221603870392,\n",
       " 0.43012621998786926,\n",
       " 0.7256380915641785,\n",
       " 0.944942057132721,\n",
       " 0.5297951698303223,\n",
       " 0.3658324182033539,\n",
       " 0.5985562801361084,\n",
       " 0.6713378429412842,\n",
       " 0.4121689200401306,\n",
       " 0.25068479776382446,\n",
       " 0.9323988556861877,\n",
       " 0.7296037077903748,\n",
       " 0.7303318977355957,\n",
       " 0.822123110294342,\n",
       " 0.8849642276763916,\n",
       " 0.993994414806366,\n",
       " 0.26326215267181396,\n",
       " 0.9361487627029419,\n",
       " 0.4664675295352936,\n",
       " 0.5143263936042786,\n",
       " 0.692293643951416,\n",
       " 0.7863380312919617,\n",
       " 0.47890016436576843,\n",
       " 0.9715637564659119,\n",
       " 0.38118043541908264,\n",
       " 0.6087024211883545,\n",
       " 0.8228999376296997,\n",
       " 0.589975893497467,\n",
       " 0.5309745073318481,\n",
       " 0.772158145904541,\n",
       " 0.16758330166339874,\n",
       " 0.5464282035827637,\n",
       " 0.7078369855880737,\n",
       " 0.5258384943008423,\n",
       " 0.8393906950950623,\n",
       " 0.2431854009628296,\n",
       " 0.9088276028633118,\n",
       " 0.5429871082305908,\n",
       " 0.5422326922416687,\n",
       " 0.21413324773311615,\n",
       " 0.37776392698287964,\n",
       " 0.9689475297927856,\n",
       " 0.8307791352272034,\n",
       " 0.9241500496864319,\n",
       " 0.46680691838264465,\n",
       " 0.15734100341796875,\n",
       " 0.2352166473865509,\n",
       " 0.7306126356124878,\n",
       " 0.7219729423522949,\n",
       " 0.1149437427520752,\n",
       " 0.22850923240184784,\n",
       " 0.7131525278091431,\n",
       " 0.13602295517921448,\n",
       " 0.6733149886131287,\n",
       " 0.4060981869697571,\n",
       " 0.9491847157478333,\n",
       " 0.553081214427948,\n",
       " 0.9304216504096985,\n",
       " 0.2599232792854309,\n",
       " 0.31619954109191895,\n",
       " 0.8989313244819641,\n",
       " 0.8614369034767151,\n",
       " 0.4110191762447357,\n",
       " 0.7190563678741455,\n",
       " 0.06268598884344101,\n",
       " 0.911842405796051,\n",
       " 0.7502488493919373,\n",
       " 0.02735048159956932,\n",
       " 0.8232752084732056,\n",
       " 0.5916017293930054,\n",
       " 0.9859434366226196,\n",
       " 0.8525948524475098,\n",
       " 0.7156773805618286,\n",
       " 0.9834852814674377,\n",
       " 0.5533037185668945,\n",
       " 0.9250403046607971,\n",
       " 0.8565834164619446,\n",
       " 0.11454936861991882,\n",
       " 0.8052797317504883,\n",
       " 0.7918348908424377,\n",
       " 0.22946913540363312,\n",
       " 0.7477061748504639,\n",
       " 0.8862618207931519,\n",
       " 0.5571216344833374,\n",
       " 0.3962804675102234,\n",
       " 0.37383386492729187,\n",
       " 0.6516942381858826,\n",
       " 0.12732498347759247,\n",
       " 0.16687770187854767,\n",
       " 0.9568377733230591,\n",
       " 0.7233127355575562,\n",
       " 0.6558277606964111,\n",
       " 0.6780432462692261,\n",
       " 0.808456301689148,\n",
       " 0.9447077512741089,\n",
       " 0.03835061937570572,\n",
       " 0.9757615923881531,\n",
       " 0.9056810140609741,\n",
       " 0.9843233227729797,\n",
       " 0.7932690382003784,\n",
       " 0.9949584603309631,\n",
       " 0.9660068154335022,\n",
       " 0.9911870360374451,\n",
       " 0.6652577519416809,\n",
       " 0.8712290525436401,\n",
       " 0.45655301213264465,\n",
       " 0.9008069634437561,\n",
       " 0.8467438817024231,\n",
       " 0.7053744792938232,\n",
       " 0.2733158469200134,\n",
       " 0.9472254514694214,\n",
       " 0.6441476941108704,\n",
       " 0.9801098108291626,\n",
       " 0.9073769450187683,\n",
       " 0.827400803565979,\n",
       " 0.794630765914917,\n",
       " 0.9321092963218689,\n",
       " 0.5187122225761414,\n",
       " 0.45979174971580505,\n",
       " 0.8846727609634399,\n",
       " 0.23621678352355957,\n",
       " 0.40790507197380066,\n",
       " 0.8725089430809021,\n",
       " 0.7739948034286499,\n",
       " 0.9332955479621887,\n",
       " 0.6360426545143127,\n",
       " 0.45416542887687683,\n",
       " 0.6730575561523438,\n",
       " 0.4929603636264801,\n",
       " 0.905994713306427,\n",
       " 0.7548593878746033,\n",
       " 0.23833052814006805,\n",
       " 0.7967063784599304,\n",
       " 0.1899203211069107,\n",
       " 0.18166297674179077,\n",
       " 0.9609418511390686,\n",
       " 0.6974179148674011,\n",
       " 0.9618426561355591,\n",
       " 0.8545200228691101,\n",
       " 0.8577041029930115,\n",
       " 0.5110160112380981,\n",
       " 0.2257421463727951,\n",
       " 0.9977431297302246,\n",
       " 0.9630394577980042,\n",
       " 0.7165914177894592,\n",
       " 0.04413481056690216,\n",
       " 0.8624394536018372,\n",
       " 0.6300233602523804,\n",
       " 0.12860319018363953,\n",
       " 0.19111667573451996,\n",
       " 0.9334660172462463,\n",
       " 0.8229926228523254,\n",
       " 0.9260011315345764,\n",
       " 0.06211942434310913,\n",
       " 0.08128257095813751,\n",
       " 0.9972608089447021,\n",
       " 0.7150770425796509,\n",
       " 0.5536234974861145,\n",
       " 0.9654965996742249,\n",
       " 0.023688213899731636,\n",
       " 0.7220967411994934,\n",
       " 0.3192591965198517,\n",
       " 0.7289338707923889,\n",
       " 0.11557159572839737,\n",
       " 0.49621158838272095,\n",
       " 0.8838049173355103,\n",
       " 0.36869484186172485,\n",
       " 0.047848235815763474,\n",
       " 0.9466108679771423,\n",
       " 0.9948790073394775,\n",
       " 0.22131313383579254,\n",
       " 0.7947480082511902,\n",
       " 0.1460503190755844,\n",
       " 0.9505947232246399,\n",
       " 0.8755621910095215,\n",
       " 0.996502161026001,\n",
       " 0.2686522901058197,\n",
       " 0.3281179964542389,\n",
       " 0.6207254528999329,\n",
       " 0.7212135195732117,\n",
       " 0.8430613279342651,\n",
       " 0.9472173452377319,\n",
       " 0.6388543844223022,\n",
       " 0.6562990546226501,\n",
       " 0.8356989622116089,\n",
       " 0.6344225406646729,\n",
       " 0.6651995778083801,\n",
       " 0.8723505139350891,\n",
       " 0.054980870336294174,\n",
       " 0.9261656999588013,\n",
       " 0.6719791889190674,\n",
       " 0.807120144367218,\n",
       " 0.8699912428855896,\n",
       " 0.6942571401596069,\n",
       " 0.6498917937278748,\n",
       " 0.7449592351913452,\n",
       " 0.7443146705627441,\n",
       " 0.4022376835346222,\n",
       " 0.9565669298171997,\n",
       " 0.1718357652425766,\n",
       " 0.9021563529968262,\n",
       " 0.7909602522850037,\n",
       " 0.7544295191764832,\n",
       " 0.8464339971542358,\n",
       " 0.2358233481645584,\n",
       " 0.9648092985153198,\n",
       " 0.7729754447937012,\n",
       " 0.309364914894104,\n",
       " 0.22039125859737396,\n",
       " 0.7115405797958374,\n",
       " 0.13083752989768982,\n",
       " 0.9677599668502808,\n",
       " 0.759501576423645,\n",
       " 0.7087928652763367,\n",
       " 0.9163775444030762,\n",
       " 0.5915670990943909,\n",
       " 0.7020223736763,\n",
       " 0.8901345729827881,\n",
       " 0.7793504595756531,\n",
       " 0.07774638384580612,\n",
       " 0.15376897156238556,\n",
       " 0.6479425430297852,\n",
       " 0.8477349877357483,\n",
       " 0.6920443773269653,\n",
       " 0.7518613338470459,\n",
       " 0.9778649806976318,\n",
       " 0.5916017293930054,\n",
       " 0.7392367720603943,\n",
       " 0.11726287752389908,\n",
       " 0.1945013552904129,\n",
       " 0.6287352442741394,\n",
       " 0.053665146231651306,\n",
       " 0.8989620804786682,\n",
       " 0.6466635465621948,\n",
       " 0.32309967279434204,\n",
       " 0.45591968297958374,\n",
       " 0.02294669672846794,\n",
       " 0.8607228994369507,\n",
       " 0.017473317682743073,\n",
       " 0.020411068573594093,\n",
       " 0.8328598141670227,\n",
       " 0.8995222449302673,\n",
       " 0.9153363108634949,\n",
       " 0.7827633619308472,\n",
       " 0.5955568552017212,\n",
       " 0.7873610854148865,\n",
       " 0.5414928793907166,\n",
       " 0.759785532951355,\n",
       " 0.1662488877773285,\n",
       " 0.22282299399375916,\n",
       " 0.9927988052368164,\n",
       " 0.8313199877738953,\n",
       " 0.0739915743470192,\n",
       " 0.6181356906890869,\n",
       " 0.6309825778007507,\n",
       " 0.15965034067630768,\n",
       " 0.6740342378616333,\n",
       " 0.7856359481811523,\n",
       " 0.23384638130664825,\n",
       " 0.8928977251052856,\n",
       " 0.43618443608283997,\n",
       " 0.9976869821548462,\n",
       " 0.40483787655830383,\n",
       " 0.7010835409164429,\n",
       " 0.6563234329223633,\n",
       " 0.41541406512260437,\n",
       " 0.6723865270614624,\n",
       " 0.9814972877502441,\n",
       " 0.9730129241943359,\n",
       " 0.6755287051200867,\n",
       " 0.9641419053077698,\n",
       " 0.5620295405387878,\n",
       " 0.5745439529418945,\n",
       " 0.9511171579360962,\n",
       " 0.990895688533783,\n",
       " 0.9565101265907288,\n",
       " 0.6007530689239502,\n",
       " 0.8674042224884033,\n",
       " 0.6795967817306519,\n",
       " 0.9165996313095093,\n",
       " 0.9421813488006592,\n",
       " 0.10935735702514648,\n",
       " 0.743218183517456,\n",
       " 0.8930066227912903,\n",
       " 0.9840725064277649,\n",
       " 0.6436671018600464,\n",
       " 0.8568648099899292,\n",
       " 0.9133679866790771,\n",
       " 0.7923417091369629,\n",
       " 0.4984266459941864,\n",
       " 0.9605535268783569,\n",
       " 0.9672154188156128,\n",
       " 0.20945987105369568,\n",
       " 0.8406564593315125,\n",
       " 0.3536394536495209,\n",
       " 0.8228665590286255,\n",
       " 0.9594032764434814,\n",
       " 0.7489604949951172,\n",
       " 0.31489020586013794,\n",
       " 0.7704999446868896,\n",
       " 0.9146517515182495,\n",
       " 0.8852987289428711,\n",
       " 0.955524742603302,\n",
       " 0.8020186424255371,\n",
       " 0.9931279420852661,\n",
       " 0.8714473247528076,\n",
       " 0.4084188640117645,\n",
       " 0.8804575800895691,\n",
       " 0.9591532945632935,\n",
       " 0.36276087164878845,\n",
       " 0.41153088212013245,\n",
       " 0.8769698143005371,\n",
       " 0.2761501669883728,\n",
       " 0.9794044494628906,\n",
       " 0.8401279449462891,\n",
       " 0.9161564707756042,\n",
       " 0.49627795815467834,\n",
       " 0.7181892395019531,\n",
       " 0.9896962642669678,\n",
       " 0.4361470937728882,\n",
       " 0.46337631344795227,\n",
       " 0.2570558786392212,\n",
       " 0.8468751311302185,\n",
       " 0.7706108093261719,\n",
       " 0.7277194261550903,\n",
       " 0.32191383838653564,\n",
       " 0.569625735282898,\n",
       " 0.7332063317298889,\n",
       " 0.7241435647010803,\n",
       " 0.6471455693244934,\n",
       " 0.7861680388450623,\n",
       " 0.9771276712417603,\n",
       " 0.500220775604248,\n",
       " 0.8116563558578491,\n",
       " 0.942396879196167,\n",
       " 0.9837439656257629,\n",
       " 0.8387796878814697,\n",
       " 0.9621490240097046,\n",
       " 0.09776417911052704,\n",
       " 0.042618244886398315,\n",
       " 0.9631682634353638,\n",
       " 0.8208695650100708,\n",
       " 0.8753725290298462,\n",
       " 0.5940247178077698,\n",
       " 0.6990429162979126,\n",
       " 0.847690761089325,\n",
       " 0.9953116774559021,\n",
       " 0.6908983588218689,\n",
       " 0.15522843599319458,\n",
       " 0.1415143758058548,\n",
       " 0.021488036960363388,\n",
       " 0.8104543685913086,\n",
       " 0.9680018424987793,\n",
       " 0.9837945103645325,\n",
       " 0.8186851739883423,\n",
       " 0.7975642085075378,\n",
       " 0.7613518238067627,\n",
       " 0.9758421182632446,\n",
       " 0.2466803789138794,\n",
       " 0.1569218933582306,\n",
       " 0.9970219731330872,\n",
       " 0.5491899251937866,\n",
       " 0.7465102672576904,\n",
       " 0.9955816864967346,\n",
       " 0.7298633456230164,\n",
       " 0.6894217133522034,\n",
       " 0.7701484560966492,\n",
       " 0.9776839017868042,\n",
       " 0.9311465620994568,\n",
       " 0.9967701435089111,\n",
       " 0.5128927230834961,\n",
       " 0.9977774024009705,\n",
       " 0.5583682656288147,\n",
       " 0.794812023639679,\n",
       " 0.938248336315155,\n",
       " 0.8012558817863464,\n",
       " 0.9660757780075073,\n",
       " 0.1898905634880066,\n",
       " 0.37077850103378296,\n",
       " 0.03684452548623085,\n",
       " 0.8316240906715393,\n",
       " 0.9956313371658325,\n",
       " 0.982991635799408,\n",
       " 0.9083858132362366,\n",
       " 0.43336057662963867,\n",
       " 0.2762094736099243,\n",
       " 0.5144358277320862,\n",
       " 0.6465815305709839,\n",
       " 0.5347818732261658,\n",
       " 0.6752755045890808,\n",
       " 0.5813538432121277,\n",
       " 0.14048023521900177,\n",
       " 0.10683095455169678,\n",
       " 0.8729245066642761,\n",
       " 0.8263511657714844,\n",
       " 0.5639135241508484,\n",
       " 0.7969485521316528,\n",
       " 0.5069901347160339,\n",
       " 0.8317368626594543,\n",
       " 0.9969049096107483,\n",
       " 0.9655523896217346,\n",
       " 0.5628515481948853,\n",
       " 0.24080972373485565,\n",
       " 0.9253537058830261,\n",
       " 0.3686145842075348,\n",
       " 0.8971561193466187,\n",
       " 0.8697905540466309,\n",
       " 0.8989771008491516,\n",
       " 0.961933434009552,\n",
       " 0.12681828439235687,\n",
       " 0.591520369052887,\n",
       " 0.8767048120498657,\n",
       " 0.47326889634132385,\n",
       " 0.9597194194793701,\n",
       " 0.4620445966720581,\n",
       " 0.7700166702270508,\n",
       " 0.23283651471138,\n",
       " 0.4808424413204193,\n",
       " 0.9631571173667908,\n",
       " 0.5553809404373169,\n",
       " 0.5614522695541382,\n",
       " 0.4603961706161499,\n",
       " 0.9814291000366211,\n",
       " 0.1356775462627411,\n",
       " 0.9791007041931152,\n",
       " 0.7908936738967896,\n",
       " 0.7990307211875916,\n",
       " 0.7955747842788696,\n",
       " 0.6996129155158997,\n",
       " 0.614676833152771,\n",
       " 0.2248384654521942,\n",
       " 0.010782941244542599,\n",
       " 0.8925877213478088,\n",
       " 0.5427901148796082,\n",
       " 0.7797120809555054,\n",
       " 0.9755762219429016,\n",
       " 0.17785537242889404,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_entailment(dataset, premises, hypotheses):\n",
    "    \"\"\"\n",
    "    Predicts the probability of entailment between two lists of sentences using DeBERTa.\n",
    "\n",
    "    Args:\n",
    "        premises: A list of strings representing the first sentences (premises).\n",
    "        hypotheses: A list of strings representing the second sentences (hypotheses).\n",
    "\n",
    "    Returns:\n",
    "        A list of entailment probabilities, one for each pair of corresponding sentences in the lists.\n",
    "    \"\"\"\n",
    "    entailment_probs = []\n",
    "    for premise, hypothesis in zip(premises, hypotheses):\n",
    "        # Preprocess sentences (tokenization)\n",
    "        inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\")\n",
    "\n",
    "        # Perform prediction\n",
    "        outputs = model(**inputs)\n",
    "        predictions = F.softmax(outputs.logits, dim=-1)  # Get probabilities for each label\n",
    "        entailment_prob = predictions[0, 0].item()\n",
    "\n",
    "        entailment_probs.append(entailment_prob)\n",
    "      \n",
    "    dataset['entailement_score'] = entailment_probs\n",
    "\n",
    "    return dataset, entailment_probs\n",
    "\n",
    "dataframe_copy = dataset_df.copy()\n",
    "dataset_df , entailment_probs = predict_entailment(dataframe_copy, context, machine_results_Finetuned_Copy)\n",
    "entailment_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 11.79ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n",
      "c:\\Users\\rachi\\work\\RAG_EVALUATION\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rachi\\.cache\\huggingface\\hub\\datasets--rachid16--ft_bert_benchmark1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/rachid16/ft_bert_benchmark1/commit/5724350ad77ae2c9b76c9161ab2c1c2c0f63a3a1', commit_message='Upload dataset', commit_description='', oid='5724350ad77ae2c9b76c9161ab2c1c2c0f63a3a1', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_dict(dataset_df)\n",
    "dataset.push_to_hub('ft_bert_benchmark1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
